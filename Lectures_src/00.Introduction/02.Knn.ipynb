{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science and Machine Learning\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img width=\"699\" alt=\"image\" src=\"https://user-images.githubusercontent.com/49638680/159042792-8510fbd1-c4ac-4a48-8320-bc6c1a49cdae.png\">\n",
    "</p>\n",
    "\n",
    "## K-Nearest neighbours\n",
    "Here we present a first example of an algorithm pipeline.\n",
    "\n",
    "The _$k$-nearest neighbours_ algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries\n",
    "\n",
    "Let's import libraries we will need in the course of the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = \"retina\"\n",
    "plt.rcParams['figure.figsize'] = (20.0, 15.0) # set default size of plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A non-parametric algorithm\n",
    "\n",
    "This is the simplest machine learning algorithm in a very specific sense: it is _non-parametric_, meaning there is no choice about the _hypothesis function_ $h_\\beta(x)$, because there is no parameter $\\beta$.\n",
    "\n",
    "### Recall: Machine Learning\n",
    "\n",
    "The general idea of machine learning is to get a model to learn trends from historical data on any topic and be able to reproduce those trends on comparable data in the future. Here is a diagram outlining the basic machine learning process:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img width=\"699\" alt=\"image\" src=\"https://files.realpython.com/media/knn_01_MLgeneral_wide.74e5e2dc1094.png\">\n",
    "</p>\n",
    "\n",
    "This graph is a visual representation of a machine learning model that is fitted onto historical data. On the left are the original observations with three variables: height, width, and shape. The shapes are stars, crosses, and triangles.\n",
    "\n",
    "The shapes are located in different areas of the graph. On the right, you see how those original observations have been translated to a decision rule. For a new observation, you need to know the width and the height to determine in which square it falls. The square in which it falls, in turn, defines which shape it is most likely to have.\n",
    "\n",
    "Many different models could be used for this task. \n",
    "\n",
    "A **model** is a mathematical formula that can be used to describe data points. \n",
    "One example is the linear model, which uses a linear function defined by the formula \n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x\\, .$$\n",
    "\n",
    "**Fitting** a model means finding the optimal values for the fixed parameters using some algorithm. \n",
    "In the linear model, the parameters are $\\beta_0$ and $\\beta_1$. \n",
    "Luckily, you won‚Äôt have to invent such estimation algorithms to get started. \n",
    "They‚Äôve already been discovered by great mathematicians.\n",
    "\n",
    "Once the model is estimated, it becomes a mathematical formula in which you can fill in values for your independent variables to make predictions for your target variable. From a high-level perspective, that‚Äôs all that happens!\n",
    "\n",
    "### Distinguishing Features of $k$NN\n",
    "\n",
    "Let's have a look at the noteworthy features of $k$NN.\n",
    "\n",
    "#### kNN is a Supervised Machine Learning Algorithm\n",
    "\n",
    "The $k$NN algorithm is a supervised machine learning model. \n",
    "That means it predicts a target variable using one or multiple independent variables.\n",
    "\n",
    "In particular, it expects to analyse data made as couples $(x, y)$ where $x$ are commonly known as _features_ while $y$ is said _target variable_.\n",
    "\n",
    "#### kNN is non-parametric\n",
    "\n",
    "As mentioned above, for knn there are no parameters to estimate.\n",
    "It is an algorithm purely based on distances, meaning that all _features_ count in the same way.\n",
    "\n",
    "### The algorithm in steps\n",
    "\n",
    "1. Define $k$\n",
    "2. Define a distance metric ‚Äî _e.g._ Euclidean distance ($2$-norm distance).\n",
    "3. For a new data point, find the $k$ nearest training points.\n",
    "4. Here it depends whether we have a classification or a regression problem.\n",
    "    - For classification, we combine neighbour classes in some way ‚Äî usually _voting_ ‚Äî to get a predicted class.\n",
    "    - For regression we combine neighbour labels by their _average_ or _median_ to get the predicted value.\n",
    " \n",
    "Let's explore these steps in details.\n",
    "\n",
    "\n",
    "The kNN algorithm is a little bit atypical as compared to other machine learning algorithms. As you saw earlier, each machine learning model has its specific formula that needs to be estimated. The specificity of the k-Nearest Neighbours algorithm is that this formula is computed not at the moment of fitting but rather at the moment of prediction. This is not the case for most other models.\n",
    "\n",
    "When a new data point arrives, the kNN algorithm, as the name indicates, will start by finding the nearest neighbours of this new data point. Then it takes the values of those neighbours and uses them as a prediction for the new data point.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img width=\"699\" alt=\"image\" src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2018/03/knn3.png\">\n",
    "</p>\n",
    "\n",
    "As an intuitive example of why this works, think of your neighbours. Your neighbours are often relatively similar to you. They are probably in the same socioeconomic class as you. Maybe they have the same type of work as you, maybe their children go to the same school as yours, and so on. But for some tasks, this kind of approach is not as useful. For instance, it would not make any sense to look at your neighbour‚Äôs favourite color to predict yours.\n",
    "\n",
    "Hence, the $k$NN algorithm is based on the assumption that you can predict the features of a data point based on the features of its neighbours. \n",
    "\n",
    "#### ‚ÄúNearest‚Äù means we need a distance\n",
    "\n",
    "$k$NN algorithm is built on the concept of _distance_. This concept has a very precise definitions in mathematics.\n",
    "\n",
    "A _distance_ $\\delta$ is a function \n",
    "\n",
    "$$ \\delta : \\Omega \\times \\Omega \\longrightarrow \\mathbb{R} $$ \n",
    "\n",
    "such that it satisfies the following properties:\n",
    "\n",
    "1. A distance is always non-negative. $\\delta(x, y) \\geq 0\\, , \\forall x, y \\in \\Omega$\n",
    "2. Separation, $\\delta(x, y) = 0 \\Leftrightarrow x = y\\, , \\forall x, y \\in \\Omega$\n",
    "3. Symmetry, $\\delta(x, y) = \\delta(y, x)\\, , \\forall x, y \\in \\Omega$\n",
    "4. Triangular Inequality, $\\delta(x, y) = \\delta(y, x)\\, , \\forall x, y \\in \\Omega$\n",
    "\n",
    "Examples of distances when $\\Omega$ is a real vector space are (among others) [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance), [Cosine distance](https://en.wikipedia.org/wiki/Cosine_similarity), [Manhattan distance](https://en.wikipedia.org/wiki/Taxicab_geometry).\n",
    "\n",
    "##### Exercise\n",
    "\n",
    "> *Implement Euclidean Cosine and Manhattan distances in Python making use of numpy.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance Exercise\n",
    "\n",
    "def euclidean_distance(x, y):\n",
    "    \"\"\"Euclidean distance implementation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x, y: np.array\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    float: the Euclidean distance between x, y.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def cosine_distance(x, y):\n",
    "    \"\"\"Cosine distance implementation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x, y: np.array\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    float: the Cosine distance between x, y.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def manhattan_distance(x, y):\n",
    "    \"\"\"Manhattan distance implementation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x, y: np.array\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    float: the Manhattan distance between x, y.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The algorithm\n",
    "\n",
    "Let's have a look at the drawing here.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img width=\"881\" alt=\"image\" src=\"https://user-images.githubusercontent.com/49638680/159125703-a6f683d0-5a03-43e2-9ae5-293c86fe4eb7.png\">\n",
    "</p>\n",
    "\n",
    "Roughly speaking: we can look the nearest data points (in this case using Euclidean distance) to the green circle (new sample $x$) and make a prediction. \n",
    "So if we look at only three neighbours (where $k = 3$) we can say that it belongs to class $1$ and if we look at the $7$ nearest neighbours ($k = 7$) we can say it belongs to class $2$.\n",
    "\n",
    "### Find the $k$ Nearest Neighbours\n",
    "\n",
    "Now that you have a way to compute the distance from any point to any point, you can use this to find the nearest neighbours of a point on which you want to make a prediction.\n",
    "\n",
    "You need to find a number of neighbours, and that number is given by $k$. The minimum value of $k$ is of course $1$. This means using only one neighbour for the prediction. The maximum is the number of data points that you have. This means using all neighbours. The value of $k$ is something that the user defines, you will see a lot of these quantities from now on. These are called _hyperparameters_. \n",
    "Cross validation procedures and optimization tools can help you with this, as you will see in the next lectures.\n",
    "\n",
    "Now, to find the nearest neighbours with respect to a point $x$ in NumPy, we need to simply apply the right function to data. As you have seen, you need to define distances on the vectors of the independent variables. \n",
    "\n",
    "Once you have the array of distances, it is enough to sort it by the magnitude of distances and pick the first $k$ elements.\n",
    "\n",
    "### Combining $k$ Nearest Neighbours labels\n",
    "\n",
    "Now, to produce predictions we need to find a way to assign a _value_ $\\hat{y}$ to the new point $x$, based on the $k$-nearest neighbours we just found.\n",
    "\n",
    "#### Classification\n",
    "If we are in a classification problem, $y_i$ are discrete values, representing classes. One method to assign a class to the new point is a procedure called _voting_.\n",
    "\n",
    "##### Voting\n",
    "**Majority Voting**: After you take the $k$ nearest neighbors, you take a ‚Äúvote‚Äù of those neighbours‚Äô classes. The new data point is classified with whatever the majority class of the neighbours is. If you are doing binary classification, it is recommended that you use an odd number of neighbors to avoid tied votes. However, in a multi-class problem, it is harder to avoid ties. A common solution to this is to decrease $k$ until the tie is broken.\n",
    "\n",
    "**Distance Weighting**: Instead of directly taking votes of the nearest neighbors, you weight each vote by the distance of that instance from the new data point. A common weighting method is\n",
    "$$\\hat{y} = \\dfrac{\\sum_i w_i y_i}{\\sum_i w_i}\\, ,$$\n",
    "where the weights $w_i := \\sum_i \\tfrac{1}{(x-x_i)^2}$. The new data point is added into the class with the largest added weight. Not only does this decrease the chances of ties, but it also reduces the effect of outliers.\n",
    "\n",
    "#### Regression\n",
    "If we are in a regression problem on the other hand, $y_i$ are continuous values. We can predict the new value $\\hat{y}$ combining $y_i$ of neighbours.\n",
    "\n",
    "**Median**: We take the median value out of the $k$-nearest neighbours.\n",
    "**Weighted average**: The weights are defined as above and we calculate $\\hat{y}$ as the weighted average of $y_i$.\n",
    "\n",
    "#### Bonus: Radius Neighbours\n",
    "\n",
    "This is the same idea as a $k$ nearest neighbour classifier, but instead of finding the $k$ nearest neighbours, you find all the neighbours within a given radius. Setting the radius requires some domain knowledge; if your points are closely packed together, you could want to use a smaller radius to avoid having nearly every point vote.\n",
    "\n",
    "## Finally: Code üë®‚Äçüíª\n",
    "\n",
    "Let's put our hands on code and let's try to implement such algorithm.\n",
    "\n",
    "### Import data\n",
    "\n",
    "We are going to import data. \n",
    "For now it is not important which data are we using. For your reference we are going to use the famous [Abalone dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/). The problem is to estimate the age of an abalone from its physical measures.\n",
    "Again, for you reference I put here an image to show you what an _abalone_ looks like.\n",
    "\n",
    "![](https://files.realpython.com/media/LivingAbalone.b5330a55f159.jfif)\n",
    "\n",
    "Indeed, the age of an abalone can be found by cutting its shell and counting the number of rings on the shell. In the Abalone Dataset, you can find the age measurements of a large number of abalones along with a lot of other physical measurements.\n",
    "\n",
    "The goal of the task is to develop a model that can predict the age of an abalone based purely on the other physical measurements. This would allow researchers to estimate the abalone‚Äôs age without having to cut its shell and count the rings.\n",
    "\n",
    "Let's apply a $k$NN model to find the closest prediction score possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url =  \"https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data\"\n",
    "\n",
    "df = pd.read_csv(url, header=None, names = [\n",
    "                                                \"Sex\",\n",
    "                                                \"Length\",\n",
    "                                                \"Diameter\",\n",
    "                                                \"Height\",\n",
    "                                                \"Whole weight\",\n",
    "                                                \"Shucked weight\",\n",
    "                                                \"Viscera weight\",\n",
    "                                                \"Shell weight\",\n",
    "                                                \"Rings\"\n",
    "                                            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole weight</th>\n",
       "      <th>Shucked weight</th>\n",
       "      <th>Viscera weight</th>\n",
       "      <th>Shell weight</th>\n",
       "      <th>Rings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.150</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.070</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.210</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.155</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.055</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sex  Length  Diameter  Height  Whole weight  Shucked weight  Viscera weight  \\\n",
       "0   M   0.455     0.365   0.095        0.5140          0.2245          0.1010   \n",
       "1   M   0.350     0.265   0.090        0.2255          0.0995          0.0485   \n",
       "2   F   0.530     0.420   0.135        0.6770          0.2565          0.1415   \n",
       "3   M   0.440     0.365   0.125        0.5160          0.2155          0.1140   \n",
       "4   I   0.330     0.255   0.080        0.2050          0.0895          0.0395   \n",
       "\n",
       "   Shell weight  Rings  \n",
       "0         0.150     15  \n",
       "1         0.070      7  \n",
       "2         0.210      9  \n",
       "3         0.155     10  \n",
       "4         0.055      7  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get rid of the `Sex` column for now, as we do not know yet how to treat this kind of data.\n",
    "\n",
    "This can be don by the `drop` method of Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('Sex', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN implementation\n",
    "\n",
    "Now we are ready to implement the algorithm. In order to do so, we need to separate _features_ and _labels_. Here we do not have the `age` column, we rather have `Rings`, which is actually the data traditionally used for age estimation.\n",
    "\n",
    "Hence this will be our label value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features matrix\n",
    "X = df.drop(\"Rings\", axis=1)\n",
    "X = X.values\n",
    "\n",
    "# label vector\n",
    "y = df[\"Rings\"]\n",
    "y = y.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have all that we need. Let's apply $k$NN with $k=3$ with respect to a new abalone with the following measures.\n",
    "\n",
    "| Variable| Value |\n",
    "|---|----|\n",
    "| Length         |  $0.569552$   |\n",
    "| Diameter       |  $0.446407$   | \n",
    "| Height         |  $0.154437$   | \n",
    "| Whole weight   |  $1.016849$   | \n",
    "| Shucked weight |  $1.016849$   | \n",
    "| Viscera weight |  $0.222526$   |\n",
    "| Shell weight   |  $0.291208$   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_point = np.array([\n",
    "     0.569552,\n",
    "     0.446407,\n",
    "     0.154437,\n",
    "     1.016849,\n",
    "     0.439051,\n",
    "     0.222526,\n",
    "     0.291208,\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to compute the distances between this new data point and each of the data points in the Abalone Dataset using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = euclidean_distance(X, new_data_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**: \n",
    "1. What do you think happens if we subtract a vector from a matrix?\n",
    "2. What shapes do `X` and `new_data_point` have?\n",
    "3. What shape do `distances` have?\n",
    "\n",
    "#### Finding the $k$ nearest neighbours\n",
    "\n",
    "You now have a vector of distances, and you need to find out which are the three closest neighbours. To do this, you need to find the indices of the minimum distances. You can use a method called `argsort()` to sort the array from lowest to highest, and you can take the first k elements to obtain the indices of the k nearest neighbours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "nearest_neighbour_ids = distances.argsort()[:k]\n",
    "nearest_neighbour_dist = distances[nearest_neighbour_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us which three neighbours are closest to your `new_data_point`. In the next paragraph, we are going to see how to convert those neighbours in an estimation for the number of rings of `new_data_point`.\n",
    "\n",
    "#### Voting or Averaging of Multiple Neighbours\n",
    "\n",
    "Having identified the indices of the three nearest neighbours of your abalone of unknown age, you now need to combine those neighbours into a prediction for your new data point.\n",
    "\n",
    "As a first step, you need to find the ground truths for those three neighbours. This is a trivial indexing exercise on numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_neighbour_rings = y[nearest_neighbour_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the values for those three neighbours, we are able to combine them into a prediction for your new data point.\n",
    "\n",
    "Our task is a regression, hence we take a weighted average of neighbours target variable. We can do this as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.025138252618467"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = np.average(nearest_neighbour_rings, weights=nearest_neighbour_dist)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "Now that you know all about the $k$NN algorithm, you are ready to start building performant predictive models in Python.\n",
    "\n",
    "##### Exercise\n",
    "\n",
    "> Use $k$NN just implemented to solve a classification problem (_e.g._ the notorious Iris classification problem)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lectures",
   "language": "python",
   "name": "lectures"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
