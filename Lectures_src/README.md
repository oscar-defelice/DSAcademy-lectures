# Lecture Plan

Here we describe the lecture plan and insert the link to the corresponding material.

---

## [Introduction](https://oscar-defelice.github.io/DSAcademy-lectures/Lectures_src/00.Introduction) üéí

Here we collect the introductory arguments for this course. In this lecture, we review some basic concepts in statistics, like probability distributions and hypothesis testing.

Hence, we illustrate the general working scheme of a _Machine Learning model_, explaining the main categories of automated learning _i.e._

- supervised learning
- unsupervised learning
- reinforcement learning

 Furthermore, we explore the main differences between predictive tasks of _classification_ and _regression_.

Hence, we show a first example of predictive model: _linear regression_.
Finally, we also focus on the general lifecycle of a Data Science model.

## [Pandas and data visualisation](https://oscar-defelice.github.io/DSAcademy-lectures/Lectures_src/01.Pandas) üêº

In this section we introduce one of the most famous basic tools for data analysis.
Furthermore, we focus on visualisation and simple data manipulation to extract insights from data.

In particular, we play with pandas dataframes, filters and matplotlib plots.

## [Data pipeline and data treatment](https://oscar-defelice.github.io/DSAcademy-lectures/Lectures_src/02.Data_preprocessing) üìä

This module is about data pipelines, data preprocessing and techniques to encode non-numerical features.
In particular we build our first data pipeline, trying to extract features to feed a machine learning model properly, guided by Exploratory Data Analysis.

## [Supervised classifier](https://oscar-defelice.github.io/DSAcademy-lectures/Lectures_src/03.Supervised_learning) ü¶æ

In this module we explore through several examples how to solve a supervised classification task.
We present logistic regression and decision tree models, furthermore we focus on the problem of predicting _probabilities_.

## [Supervised regressor](https://oscar-defelice.github.io/DSAcademy-lectures/Lectures_src/03.Supervised_learning) ü¶ø

We keep studying the linear regression from a different point of view.
Hence, we introduce polynomial regression and we move on to predict continuous numerical values.

## [Regularisation](https://oscar-defelice.github.io/DSAcademy-lectures/Lectures_src/04.Regularisation) üåä

At this stage, our Machine Learning knowledge is mature enough to wonder how to measure model performances and how to solve the possible issues arising.
We introduce the crucial concepts of _underfit_ and _overfit_ and how to face such problems that may afflict models.

## [Software and Tools](https://oscar-defelice.github.io/DSAcademy-lectures/Lectures_src/05.Software_tools) üîß

This lecture is aimed to introduce some powerful tools:

- [Tableau](https://www.tableau.com/products/public/) for data visualisation and dashboards.
- [Git and GitHub](https://www.github.com/) for code versioning and collaboration.

## [Time Series](https://oscar-defelice.github.io/DSAcademy-lectures/Lectures_src/06.Time_series) ‚åõ

We illustrate methods to perform time series analysis and decomposition.
We give also mention of techniques to time series forecast.

## [Unsupervised learning](https://oscar-defelice.github.io/DSAcademy-lectures/Lectures_src/07.Unsupervised_learning) ü§ñ

In this lecture, we drop labels from our data and try to recognise patterns in them.
We will find that unsupervised learning tasks are capable of surprising learning schemes.

## [Ensemble methods](https://oscar-defelice.github.io/DSAcademy-lectures/Lectures_src/08.Ensemble_methods) üå≥üå≥üå≥

As the last lecture, we focus on techniques to take advantage of many models in order to make predictions more robust and reliable.
In particular, we present _random forest_, bagging and boosting techniques and the notorious _XGBOOST_.

## [Final project](https://oscar-defelice.github.io/DSAcademy-lectures/Lectures_src/09.Final_project) üñºÔ∏è

The proposed final project is an application of what we have seen in the course of the lectures.

The idea is to build a whole pipeline from data collection to prediction.
